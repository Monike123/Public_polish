DATASET ANALYSIS REPORT
============================================================

Dataset Shape: (672, 15)
Total Missing Values: 0
Duplicate Rows: 0

COLUMN DETAILS:
----------------------------------------

index (int64):
  Non-null: 672
  Unique: 672
  Count: 672
  Mean: 335.5
  Std: 194.13397435791603
  Min/Q1/Median/Q3/Max: 0.0/167.75/335.5/503.25/671.0

Job Title (object):
  Non-null: 672
  Unique: 172
  Top values: {'Data Scientist': 337, 'Data Engineer': 26, 'Senior Data Scientist': 19, 'Machine Learning Engineer': 16, 'Data Analyst': 12, 'Senior Data Analyst': 6, 'Senior Data Engineer': 5, 'ENGINEER - COMPUTER SCIENTIST - RESEARCH COMPUTER SCIENTIST - SIGNAL PROCESSING - SAN ANTONIO OR': 4, 'Data Science Software Engineer': 4, 'Data Scientist - TS/SCI FSP or CI Required': 4}

Salary Estimate (object):
  Non-null: 672
  Unique: 30
  Top values: {'$75K-$131K (Glassdoor est.)': 32, '$79K-$131K (Glassdoor est.)': 32, '$99K-$132K (Glassdoor est.)': 32, '$137K-$171K (Glassdoor est.)': 30, '$90K-$109K (Glassdoor est.)': 30, '$56K-$97K (Glassdoor est.)': 22, '$90K-$124K (Glassdoor est.)': 22, '$79K-$106K (Glassdoor est.)': 22, '$101K-$165K (Glassdoor est.)': 21, '$91K-$150K (Glassdoor est.)': 21}

Job Description (object):
  Non-null: 672
  Unique: 489
  Top values: {'Job Overview: The Data Scientist is a key member of our cross-functional Product team responsible for discovering new and innovative solutions to the challenges within the built environment. Now, more than ever, building owners and operators rely on Hatch Data to get a birds-eye view of asset performance to help them achieve ambitious goals around sustainable financial, environmental, and social impact. We are actively seeking an exceptional Data Scientist to help us take one of the worlds most extensive data sets for commercial building operations and transform it into amazing products for our customers.\n\nResponsibilities and Duties:\nPartner with Product Management, to Investigate high-level questions to discover new relationships between operations and outcomes\nMaintain and improve upon the accuracy and utility of existing machine-learning algorithms\nOwn the long-term Data Science Roadmap, including prototype solutions in "green field" problem spaces\nSupport ad-hoc data analysis in support of customer success and marketing efforts\nCollaborate closely with domain experts in energy and mechanical engineering, as well as with the software engineering team\nCoach and mentor others to amplify your impact and continuously demonstrate the value of data science\nExperience:\nBachelors degree in Data Science, Computer Science, Math, or related technical field.\nMinimum 3+ years experience applying data science techniques to drive product development and decision-making.\nDemonstrated track record of solving problems in industry with machine learning\nExpertise in statistical methods and experimental design and analysis\nFluent in at least one modern language for data processing (R, Python, and Scala experience ideal).\nComfortable with AWS and distributed data processing systems (e.g., Spark)\nFamiliarity with energy-efficiency, commercial building domain, regression and baseline techniques, weather normalization, etc. a plus\nExperience using APIs to manipulation and move data in and out of systems.\nMust possess outstanding written and verbal communication skills in English.\nAbility to prioritize development and operational tasks in a dynamic and challenging environment.\nAbout You:\nCare about leaving the world better than you found it.\nMission-driven and leads by example with equal parts brain and heart.\nOperationally-minded, empathetic, self-starter, driven, and gets things done.\nNatural communicator that instills confidence and trust with internal and external stakeholders.\nExcited to join a small company, with big company benefits, and upside potential.\nIdeally based in the San Francisco Bay Area.\nAbout Us:\n\nClimate change is real. Addressing this is the biggest issue of our time. While moving the world from dirty energy sources to clean ones matters, there is also an enormous opportunity to use existing resources more efficiently. Buildings consume up to 40% of the energy in North America alone. This is a big opportunity to create change. We bring our values to work and every day push to solve these problems at scale.\n\nOur team applies their knowledge of energy systems, mechanical engineering, IoT, machine learning and the built environment, to design cutting edge software that reduces the negative environmental impact of our indoor spaces while improving the bottom line for owners and operators. Our software platform has been proven across more than 400M square feet of commercial real estate with marquee customers, and were doubling down our efforts in the coming months as we prepare to accelerate growth and impact.\n\nThis is a unique opportunity to join a company that does well by doing good and is positioned for significant expansion. Youll enjoy comprehensive benefits, competitive salary, and meaningful equity. We offer a flexible work environment and the opportunity to work alongside an incredibly talented, fun, and highly motivated team.\n\nHow to Apply:\n\nDoes this sound like you? If so, please submit your resume and cover letter and provide any context you think would be helpful as we learn about you. Well be interested in understanding why youre excited about joining us, why this role seems like a perfect fit for your talents, and what success looks like for you, personally, over the next few years.': 12, "We love programming and the excitement that comes with building something people use. We are the kind of people that love talking to users and can find the balance between solving a problem quickly and thinking about how your code will work in the future. We love to move fast, keep learning and get stuff done.\n\nOur data science team is still in its early days and you'll have a big impact on our direction and how we operate. You'll be central to researching, developing and shipping products that help our customers learn and grow from their data. For this role, we're looking for people who have developed split brains--software engineers who have become great at writing machine learning code or data scientists who have become great software developers.\n\nTechnologies we use (not comprehensive!):\nPython\n\nNumpy, Scipy, Pandas\n\nAurora, Cassandra, Kafka\n\nHTML, JavaScript, React\n\nSageMaker\n\nHow you will make a difference:\n\n\nAnalyze large data sets (we're collecting billions of individual actions every month).\nBuild products that enable our customers to grow faster and communicate more effectively with their customers.\nDevelop machine learning models and pipelines for research and production.\n\nWho You Are:\n\n\nHave experience implementing machine learning models, data pipelines and testing frameworks for research and production use.\nHave demonstrated a measurable impact based on the models you've created. It's not always easy getting a model correct, we love talking about places we got stuck and work as a team to think through ideas that could unblock us.\nHave experience processing cloud-scale data using parallel, elastic, streaming and similar techniques.\nEnjoy tuning and validating machine learning models and take a rigorous approach.\nUnderstand how to profile code and optimize performance.\nAspire to correctness (e.g. in your code, in drawing conclusions from data)\nHave a bachelor's or advanced degree in computer science, applied math, statistics or other relevant quantitative discipline, or equivalent industry experience.\n\nGet to know Klaviyo\n\nKlaviyo is the world's leading owned marketing platform known for accelerating revenue for online businesses using the channels they own like email, web, and mobile. Enabling companies to leverage these owned marketing channels, Klaviyo makes it easy to store, access, analyze and use transactional and behavioral data to power highly-targeted customer and prospect communications. And unlike other marketing platforms, Klaviyo doesn't force companies to compromise between advanced functionality or ease of use - so companies of all sizes are able to maximize their sales quickly. That's why over 28,000+ innovative companies like Unilever, Custom Ink and Eventbrite sell more with Klaviyo.": 5, "Join our Defense and Intelligence Solutions Division to pursue the exciting new area of developing the avionics applications and next generation of cyber threat detection capabilities for airborne platforms. Collaborate in a dynamic team environment, developing and testing software for various types of aircraft. Responsibilities and activities will include software development tasks primarily utilizing C/C++. The target platform will be high performance, fault tolerant applications on embedded platforms. Participate in requirements analysis, software design, coding, unit testing, integration, installation and maintenance. Develop new ideas and solve complex problems based on technical and domain experience. Work in teams as well as independently. Work at SwRI laboratories and short-term travel to government facilities / military bases or other customer facilities. Prepare technical documents and support proposal preparations as required. Work assignment is in San Antonio or Oklahoma City.\n\n\nEducation/Experience:\n\n\nRequires a BS degree in Computer Science, Computer Engineering (must have focus in Computer Science) with 0-5 years of experience. Must have at least a 3.0 GPA. Must have experience or coursework with software development with emphasis on application development utilizing C/C++. Must have the basic knowledge of software development processes including an ability to analyze and translate requirements into an application design, implementation, unit and integration testing, software/hardware integration and software documentation. Exposure to embedded real-time application development as well as cyber detection will be considered. Experience with Python and/or Java desirable. Exposure to agile methodologies, software engineering practices, requirements analysis, software design are desirable. Must have excellent verbal and written communications skills; specially technical communication. Current DoD security clearance is highly desired. A valid/clear driver's license is required.\n\n\nSpecial Considerations:\n\n\nApplicant selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information. Applicant must be a U.S. citizen.\n\nJob Locations: San Antonio, Texas | Oklahoma City, Oklahoma\n\n\nFor more information about this division, visit the Defense & Intelligence Solutions home page.\n\nAn Equal Employment Opportunity/Affirmative Action Employer\nRace/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disabled/Veteran\nCommitted to Diversity in the Workplace": 4, "US Citizenship Required and (TS/SCI with FSP or CI) Required\n\nJob Description:\nExcited about data? Take action with it.\n\nAs a Phoenix Data Scientist, you'll be part of a team with access to tons of real mission and that takes action on it. You will not be simply designing algorithms or analyzing theoretical use cases. You'll be using languages like R, Python and Scala to glean actionable insights and build tangible capabilities.\n\nOn a typical day as a Phoenix Data Scientist, you'll collaborate with both engineers and mission SMEs (e.g. Cybersecurity or SIGINT analysts) to process mass amounts of data. You'll implement (not just design) neural networks, Bayesian models and other Machine Learning techniques to address mission needs.\n\nMake a difference today.\nSkills Required:\nPython or Scala\nExperience in a Linux environment\nComfortable with very large data sets\nSkills Preferred:\nR, Rstudio, Shiny\nApache Spark\nMachine Learning\nNeural Networks\nArtificial Intelligence\nPredictive Modeling\nQualifications:\nUS Citizenship Required\nSecurity Clearance (TS/SCI) Required\nSome positions require FSP\nBachelors Degree in Math, Science or Engineering\n\nBenefits Offered:\nMedical, Dental, Vision Insurance - 100% Company Paid Premiums\nSTD, LTD, and Life Insurance - 100% Company paid\n401K Automatic 10% company contribution; no matching required\nPTO - 4 weeks/year\nHolidays - 10 paid/year\nBirthdays off with pay\nReferral Bonuses Upfront AND Annually Recurring\nOpen Source Bonuses Contribute to our Github projects\nProfessional Development Paid training, Certifications and Enrichment\nABOUT PHOENIX OPERATIONS GROUP:\nPhoenix Operations Group is a high-end engineering services company dedicated to protecting and advancing our national cyber resources. As a small start-up operating out of the Baltimore/Washington DC area, we rely on innovation to continually advance our employees' skills and provide game-changing solutions to our customers.\n\nOur technical competencies include Big Data analytics (batch and streaming), Cloud Computing infrastructure, multi-INT visualization, and enterprise architectures. We support operational missions (All-Source, Financial, CND) and serve as Product Owners for our open-source research initiatives.\n\nPlease visit us at http://www.phoenix-opsgroup.com for more information.\n\nPhoenix Operations Group is an Equal Opportunity Employer. Phoenix Operations Group does not discriminate on the basis of race, religion, color, sex, gender, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.": 4, 'Do you have a head for numbers? Like turning real-world problems into mathematical representations and checking to see if you did it right? If you like that, youll love developing algorithms with us! Our customers have some of the most interesting needs and hardest challenges to resolve. Our solutions require integrated models based on large and varied datasets, feeding analytic applications so that we can extract patterns and behaviors obscured within the data to augment the capabilities of human operators. We need both experienced and aspiring data scientists to work on all stages of the data science pipeline, to include applying existing machine learning software across large hyper-parameter trade spaces, developing new machine learning algorithms and evaluating the best the commercial AI/ML industry has to offer to our problem set.\n\nWhy Maxar?\nWe build advanced algorithms to gain analytic insights from a large range of open source and government data.\nWe enable machine learning systems, automate workflow, and design and develop custom applications for unique national-security mission.\nWe operate an end-to-end predictive analytic platform unlike any other within the US Government.\nWe provide training to expand your skills and challenges to develop them.\nOur clients missions are vital to national security, so were mission-first always.\nOur work environment is relaxed business casual.\nAt our core we believe and practice social responsibility.\nWhat would you be doing?\nExtracting and transforming data using programming languages such as Java and Python and associated open source data analytics libraries.\nAscertaining unique ways to apply algorithms to derive specific customer data analytic results.\nApplying big data analytics tools to large, diverse sets of collection data to assess risk of adverse threat activities.\nExtending existing algorithms as required to support customer requirements.\nApplying data science methods to create and optimize convolutional neural networks that will perform classification, localization and segmentation of various modalities of remotely-sensed imagery.\n\nMinimum Qualifications:\nMust be a US citizen with a current/active TS/SCI and be willing and able to obtain a CI Polygraph.\nB.S. in Engineering, Math, Physics, Computer Science or related field.\n8 years of professional experience as a Data Scientist.\nExperience with at least one of these languages: R, VBA, Java, C++, SQL, Python.\n\nDesired Qualifications:\nGraduate experience working with probabilistic and stochastic statistical analysis or computational intelligence.\nExperience conducting model feasibility research and algorithm development for machine learning.\nExperience with distributed datasets and experience analyzing both relational and NoSQL data structures.\nExperience working in an Agile environment, especially SAFe.\nAscertaining unique ways to apply algorithms to derive specific customer specific data analytics results.\nApplying big data analytics tools to large, diverse sets of collection data to assess risk of adverse threat activities.\nExtending existing algorithms as required to support customer requirements.\nExperience implementing algorithms related to machine learning.\nKnowledge of technical aspects of ISR systems.\nExperience conducting model feasibility research and algorithm development for machine learning.\nExperience developing and testing models.\nExtracting and transforming data using programming languages such as Java and Python and associated open-source data analytics libraries.\nExperience with Data Analytics.\nExceptional oral and written communications.\nOrganizational skills and excellent attention to detail.\nDevelopment experience in a Linux/Unix/Windows environment.\nCapability to work effectively in a geographically distributed development team.\nBasic systems administration and installation.\nBuild-test-deploy frameworks (Jenkins, GoCD, etc.).\nContainerization platforms (Docker, OpenShift, CloudFoundry, etc.).\nOccasional local travel to government sites for customer meetings and demonstrations.\n\n#cjpost': 4, "Please review the job details below.\nWe are looking to hire a Data Scientist. The selected candidate will complete data science projects on a short-term, quick-reaction basis aligned with the Sponsor’s requirements.\n\nHighlights of Responsibilities:\nArrange and participate in meetings to identify and document project requirements.\nDevelop scripts and software programs to address requirements using programming languages, tools and computational methods.\nLeverage and implement appropriate mathematics and statistics methods.\nCreate software interfaces and visualizations to convey the results of advanced analytics projects.\nCreate a repository and process for cleaning, cataloging and storing data.\nDevelop and use entity resolution and network analysis techniques.\nCreate machine learning training data-sets and deep learning models.\nAutomate existing workflows.\nDevelop tools for dissemination.\nEvaluate outputs of existing algorithms with data science tools and methods.\nBuild, test and deploy new automated approaches that identify activity of interest in large, complex data-sets.\nMinimum Qualifications:\nActive/current TS/SCI security clearance with Polygraph.\nBachelor's Degree. (4 years of experience may be substituted in lieu of a degree).\n10 years of experience in data science or a related field.\nDemonstrated knowledge of applied mathematics, computer programming and visualization.\nAbility to develop comprehensive software applications.\nAbility to work collaboratively and effectively in a team environment.\nPreferred Qualifications:\nDemonstrated knowledge of research designs.\nDemonstrated knowledge of collection methods, capabilities and tasking process.\nDemonstrated knowledge of issues of national concern.\nDemonstrated knowledge of Python and some of the following software/tools: SQL, R, Hadoop, Spark, Java, C/C++, Git, Bash, Tableau, ArcGIS, Unix commands\n#cjpost\n\nMAXAR Technologies values diversity in the workplace and is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.": 3, 'IZEA was built to connect the world’s top brands with influential content creators and publishers to enable influencer marketing and content production at scale. With over 500,000 Creators reaching over 3 billion fans and followers around the globe, IZEA is unmatched in its industry experience, network diversity, and technology ecosystem. A career at IZEA offers countless ways to make an impact in a fast-growing organization!\n\nIZEA is looking for a Data Engineer to join our Core Technology group.\n\nThis team member may be located in the Orlando area or remote.\n\nYou will use the latest tools and technology to build and support IZEA\'s industry-leading data initiatives. Whether contributing to our data pipelines that ingest millions of records per hour, distilling vast amounts of data into meaningful insights, or operating the services that surface those insights, you will help define what the future of Influencer and Content Marketing looks like.\n\nYou will have direct access to end-users and stakeholders, and you are encouraged to build and leverage these relationships in your work. You will write and test your code, and work with our QA team to get it deployed to production. With the help of a homegrown, bot-driven CI/CD pipeline, your code will be delivered to users daily. This cross-functional team leverages Amazon Web Services for everything from ECS for containerized virtualization and hosting, to S3 for durable object storage, RDS and DynamoDB for persistence, EMR for the batch processing of billions of records, and Lambda for distributed workloads and stream consumption.\n\nAside from the day to day, we can offer you incredible benefits including an annual continuing education budget, a trust-focused development process, a flexible and collaborative work environment where balance matters, stock ownership, and an annual company retreat.\n\nThe team strives to be ego-free and motivated only by building amazing software for our users. We seek to understand the "why" behind the "what". We regularly break out into small teams to tackle problems, learn new technologies, or just share what we know with others. We test our code and invest in the health of our systems. We push each other, learn from each other, and strive to continually grow.\n\nPrimary Responsibilities\n\nYou will...\nWork with stakeholders to define the solutions to development problems and business requirements\nDevelop and maintain the features and capabilities of our data ingestion pipelines\nExtract actionable and impactful insights from vast amounts of data\nDevelop and maintain the services that surface those insights and make them available for consumption in a performant manner\nCreate unit and integration tests for your code\nReproduce and fix bugs reported by internal and external users\nSet goals and communicate often about your progress toward them\nContribute to the ongoing improvement of the engineering organization and our software\n\nWhat We’re Looking For\n\nA problem solver at heart\nMuch of our work revolves around problems that have no existing off-the-shelf solution or consensus on best practices. You\'ll often need to break down large problems into smaller more manageable tasks and utilize critical thinking to come up with novel ideas.\n\n3+ years of engineering experience\nThings move quickly in the data group. You’ll need to be comfortable and familiar with delivering highly scalable cloud based applications.\n\nProfessional experience with frameworks like Hadoop, Spark, or similar\nThis will be a significant component of your responsibilities. You will be working with large data sets in a distributed environment on a daily basis. Familiarity w/ the Hadoop/Spark ecosystem is a must.\n\nProfessional experience with Python data tools\nOur data team uses PySpark and Jupyter notebooks extensively. Familiarity with these technologies will also serve you well.\n\nDirect experience with relational and NoSQL databases technologies\nWe use the best tool for the job around here. When it comes to storing and accessing data, we recognize that the technology decisions we make directly impact our ability to provide a performant customer experience, and our own costs.\n\nExperience designing and building JSON based RESTful APIs\nBecause we are building our application with a front end framework, we carefully design and document the APIs to power it. To help us, we follow the JSON API spec, but any experience in building a RESTful API will be useful. Remember, the API is your contract with the consumers of your data!\n\nUnderstanding of monolithic and micro-service based architectures\nIZEAx still has some legacy monolithic characteristics. We move more and more of our technology to a distributed set of services, there are new challenges to overcome. Understanding the differences between these two models will help you take those challenges head on.\n\nBasic Linux skills\nIn order to develop our data pipelines and services, you need to run it on your laptop. This means opening up some terminal windows, running some commands, and keeping the log output open. Additionally, some of our technology stack is better accessed through CLIs. Examples include the Ember CLI, the Rails CLI/console, Docker commands, Gradle, and our own CLIs. We\'ll walk you through it, but you should be comfy in a terminal.\n\nAbility to multitask and prioritize multiple incoming requests\nIZEA\'s Engineering team strives to provide a great experience and great service to our users. In order to do that, you may need to context switch into a support issue or drop what you are doing to start work on something else. This is part of what Agile means to us.\n\nExcellent verbal and written communications skills\nRegular and timely communication is the key to a trust based development process. You should be able to simply and concisely ask for feedback and direction in terms that your audience understands, and relay requested information in a timely fashion to your leaders. You should prioritize documentation of processes and code.\n\nDemonstrated experience with the following will be highly valued\n\nData Science Background\nWhile not required, a fundamental understanding of statistics and modeling would be a great asset. Practical experience with Machine Learning and/or complex data pipelines would also be welcome.\n\nFront End development experience\nFrom time to time you may need to build visualization, or (lite) user facing experiences. Familiarity with modern web frameworks like Angular, React, Ember, or Vue would be helpful. Extra bonus points if you have familiarity w/ Javascript based visualization libraries like d3.js or Highcharts.\n\nAmazon Web Services, or other cloud providers\nIZEA\'s software is hosted on AWS, and you will need to acquire some familiarity with it. Previous experience in using a cloud provider, even if just for developer tooling, shows that you understand some of the nuances involved in working in the cloud. Familiarity with Amazon’s EMR would also be an asset.\n\nContinuous Integration & Deployment\nIZEA needs to get features and fixes out to customers as soon as we possibly can with as much confidence as possible. To facilitate this, we have developed a CI/CD pipeline (using 3rd party services). An understanding of what CI/CD is will help you understand how this pipeline works and how to make it even better.\n\nGitHub\nAll of IZEA\'s code is source controlled on Github. We leverage Github Pull Requests for code reviews, Github integrations manage parts of our CI/CD pipeline, and Github releases define the code tags that ultimately get deployed. Much of our process documentation exists on Github pages. Familiarity with navigating Github\'s features will help you ramp up in our SDLC faster.\n\nJIRA\nIZEA uses JIRA to manage projects and report on progress to stakeholders inside and outside the company. While we strive to automate as much of JIRA as possible with bots, webhooks and reports, understanding how JIRA issues, links, attachments, and workflows work will help you understand our SDLC faster.\n\nAbout IZEA:\n\nWe are IZEA: The Creator Marketplace. Our cloud-based technologies connect Brands and Publishers with content Creators who blog, tweet, pin, and post on their behalf.\n\nOur driving belief is that the only way to thrive in our rapidly changing world is to change ahead of it. IZEA is in a constant state of evolution and reinvention. While we may have invented the industry, we still operate like an entrepreneurial, scrappy start-up. Your time here will be exciting, educational, and at times, a bit crazy.\n\nWith IZEA, you have the opportunity to join a non-traditional corporate culture, where creativity and productivity are valued over a suit and tie. We call it "The IZEA Way."\n\n\nWhy would you want to work here?\n\nOur developers use the latest tools and technology to build the applications and services that make up the IZEA Exchange platform. We write in whatever language we need to get the job done including Ruby, Java, Python, PHP, Swift, and Javascript. We leverage the latest frameworks, such as Rails, Symphony2, and Drop Wizard for iterating quickly on our technology.\n\nOur code is deployed and operated by the people who write it, with the help of Amazon Web Services. We leverage everything from EC2 for virtualization and hosting, to Amazon EMR and Machine Learning for advanced analytics.\n\nAside from the day to day, we offer incredible benefits including an annual continuing education budget, a flexible trust-focused development process, and an open collaborative work environment.\n\nOur team is ego-free and motivated to build great software. We regularly break out into small teams to tackle problems, learn new technologies, or just share what we know with others. We test our code and invest in the health of our systems. We push each other, learn from each other, and strive to continually grow as a team.\n\nCalifornia residents, please follow this link to view the types of information we may gather from California residents who are applicants, employees, or contractors of IZEA, and how we use such information.': 3, "US Citizenship Required and (TS or TS/SCI) Required\n\nJob Description:\n\nAre you ready for a position that really matters and is not just about corporate profits? As a Phoenix Data Scientist, you'll be part of a small team composed of both engineers and mission subject matter experts (SMEs). This team will build Machine Learning solutions augmenting human analysts inundated with information overload in carrying out critical Intelligence and Defense missions. Youll use Python and distributed computing technologies to build these solutions in an iterative (Agile) fashion.\n\nOn a typical day, youll be working either on a classified network or working remotely from home on an unclassified network. Youll participate in Scrum meetings, collaborate with SMEs (e.g. SIGINT, ELINT, All-Source) to understand the mission space, explore / characterize / process data sets, and implement analytics with support from Data Engineers & Software Engineers.\n\nSkills Required:\nPython (expert-level)\n3+ years of experience as a Data Scientist\n3+ years of experience in a Linux environment\n2+ years of experience using distributed (cluster) technology\n1+ years of experience using Jupyter Notebook\nSkills Preferred:\nMachine Learning\nApache Spark\nApache Kafka\nApache NiFi\nDocker\nExperience with the following python libs: pandas, geopandas, geopy, sklearn, sklearn.cluster, IPython, shapely, scipy, numpy.\nQualifications:\nUS Citizenship Required\nSecurity Clearance (TS/SCI) Required\nBachelors Degree in Math, Science or Engineering\nABOUT PHOENIX OPERATIONS GROUP:\nPhoenix Operations Group is a high-end engineering services company dedicated to protecting and advancing our national cyber resources. As a small start-up operating out of the Baltimore/Washington DC area, we rely on innovation to continually advance our employees' skills and provide game-changing solutions to our customers.\n\nOur technical competencies include Big Data analytics (batch and streaming), Cloud Computing infrastructure, multi-INT visualization, and enterprise architectures. We support operational missions (All-Source, Financial, CND) and serve as Product Owners for our open-source research initiatives.\n\nPlease visit us at http://www.phoenix-opsgroup.com for more information.\n\nPhoenix Operations Group is an Equal Opportunity Employer. Phoenix Operations Group does not discriminate on the basis of race, religion, color, sex, gender, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.": 3, 'About Rocket Lawyer\nWe believe everyone deserves access to simple and affordable legal services.\n\nFounded in 2008, Rocket Lawyer is the largest and most widely used online legal service platform in the world. With offices in North America and Europe, Rocket Lawyer has helped over 20 million people create over 50 million legal documents, and get their legal questions answered.\n\nWe are in a unique position to enhance and expand the Rocket Lawyer platform to a scale never seen before in the company’s history, to capture audiences worldwide. We are expanding our team to take on this challenge!\n\nAbout the Role\nRocket Lawyer is looking for a Data Engineer that will contribute in all aspects of creating an analytical data driven environment. The core data engineering team is responsible for the building out the data pipeline, gathering internal and external data, generating metrics, managing and monitoring batch and streaming jobs, and implementing analytical tools to drive strategic decision making.\nA Day in the Life\nEvangelize Modern Big Data Practices\nDesign warehouse schemas that accurately represent our business, and facilitate analysis and building of reports\nHelp build batch and streaming data ingestion pipeline using Hadoop, Hive, Pig, Storm, and Kafka Streams\nWrite ETL jobs to transform raw data into business information to drive decision making\nDevelop analytical environment using internal and external reporting tools\nIntegrate internal and external data with warehouse and external tools\nExperience\nExcellent technical skills including expert knowledge of the Hadoop ecosystem\nExperience of the analysis, design and development of Data Warehouse and Big Data solutions, including analyzing source systems, developing ETL design patterns and templates, ETL development, data profiling and data quality issues resolution.\nProject and team management experience\nExcellent communication skills and presentation skills\nStrong SQL, Java, and Python skills\nDatabase (relational & NoSQL), Data Warehouse knowledge\nStream processing experience (Storm, Kafka Streams)\nPassion and enthusiasm for learning new technologies and technique\nComfortable with Linux\nBS or MS in computer science\nDetail oriented and organized\nDesire to learn broad set of technologies\nBenefits and Perks\nComprehensive health plans (including Medical, Dental and Vision insurance for full-time employees)\nUnlimited PTO\nCompetitive salary packages\n401k program\nLife insurance\nDisability benefits\nFlexible Spending Accounts\nCommuter/Transit Program\nYour choice of a MAC or PC\nMonthly onsite masseuse sessions\nWeekly Friday catered lunches\nCompany sponsored events, both on- and off-site': 3, '15-Apr-2020\n\nJob ID\n279187BR\n{"QuestionName":"Job Description","AnswerValue":"750 million. That’s how many lives our products touch. And while we’re proud of that fact, in this world of digital and technological transformation, we must also ask ourselves this: how can we continue to improve and extend even more people’s lives?\n\nWe believe the answers are found when curious, courageous and collaborative people like you are brought together in an inspiring environment. Where you’re given opportunities to explore the power of digital and data. Where you’re empowered to risk failure by taking smart risks, and where you’re surrounded by people who share your determination to tackle the world’s toughest medical challenges.\n\nWe are Novartis. Join us and help us re-imagine medicine.\n\nTwo companies and one incredible alliance. Novartis and Microsoft have formed alliance to leverage data & Artificial Intelligence (AI) to develop trans-formative medicines faster and more cost-effectively for patients worldwide. We are seeking a thought leader and team builder to join the Novartis Innovation AI Lab to advance the field of Life Science and healthcare analytics. In this newly formed alliance with Microsoft, you will lead analysis of large image data sets for Novartis.\nYour responsibilities:\nIn this newly created role, you will:\n• Lead independently end-to-end analysis of image data sets\n• Take a hands-on role and deliver on multiple highly visible data science projects\n• Serve as an ambassador for Novartis Data Science by presenting and publishing articles at conferences, business meetings and academic institutions\n• Facilitate design and creation of knowledge repositories\n• Keep ahead of latest development in the field and mentor associates\n• Collaborate with the digital and DSAI teams\n• Inspire others on culture change\n\nPosting Title\nSenior Data Scientist – Image Analytics, Novartis AI Innovation Lab': 3}

Rating (float64):
  Non-null: 672
  Unique: 32
  Count: 672
  Mean: 3.5186011904761907
  Std: 1.4103288795924833
  Min/Q1/Median/Q3/Max: -1.0/3.3/3.8/4.3/5.0

Company Name (object):
  Non-null: 672
  Unique: 432
  Top values: {'Maxar Technologies\n3.5': 12, 'Hatch Data Inc': 12, 'Tempus Labs\n3.3': 11, 'AstraZeneca\n4.0': 10, 'Klaviyo\n4.8': 8, 'Phoenix Operations Group\n5.0': 7, 'Autodesk\n4.0': 7, 'Novetta\n4.5': 6, 'Southwest Research Institute\n3.9': 6, 'MassMutual\n3.7': 5}

Location (object):
  Non-null: 672
  Unique: 207
  Top values: {'San Francisco, CA': 69, 'New York, NY': 50, 'Washington, DC': 26, 'Boston, MA': 24, 'Chicago, IL': 22, 'Herndon, VA': 21, 'Cambridge, MA': 18, 'McLean, VA': 12, 'United States': 11, 'Chantilly, VA': 11}

Headquarters (object):
  Non-null: 672
  Unique: 229
  Top values: {'New York, NY': 33, 'San Francisco, CA': 31, '-1': 31, 'Chicago, IL': 23, 'Boston, MA': 19, 'Reston, VA': 14, 'Mc Lean, VA': 13, 'Westminster, CO': 12, 'Cambridge, MA': 10, 'Cambridge, United Kingdom': 10}

Size (object):
  Non-null: 672
  Unique: 9
  Top values: {'51 to 200 employees': 135, '1001 to 5000 employees': 104, '1 to 50 employees': 86, '201 to 500 employees': 85, '10000+ employees': 80, '501 to 1000 employees': 77, '5001 to 10000 employees': 61, '-1': 27, 'Unknown': 17}

Founded (int64):
  Non-null: 672
  Unique: 103
  Count: 672
  Mean: 1635.529761904762
  Std: 756.7466402326282
  Min/Q1/Median/Q3/Max: -1.0/1917.75/1995.0/2009.0/2019.0

Type of ownership (object):
  Non-null: 672
  Unique: 13
  Top values: {'Company - Private': 397, 'Company - Public': 153, 'Nonprofit Organization': 36, 'Subsidiary or Business Segment': 28, '-1': 27, 'Government': 10, 'Other Organization': 5, 'Private Practice / Firm': 4, 'Unknown': 4, 'College / University': 3}

Industry (object):
  Non-null: 672
  Unique: 58
  Top values: {'-1': 71, 'Biotech & Pharmaceuticals': 66, 'IT Services': 61, 'Computer Hardware & Software': 57, 'Aerospace & Defense': 46, 'Enterprise Software & Network Solutions': 43, 'Consulting': 38, 'Staffing & Outsourcing': 36, 'Insurance Carriers': 28, 'Internet': 27}

Sector (object):
  Non-null: 672
  Unique: 23
  Top values: {'Information Technology': 188, 'Business Services': 120, '-1': 71, 'Biotech & Pharmaceuticals': 66, 'Aerospace & Defense': 46, 'Finance': 33, 'Insurance': 32, 'Manufacturing': 23, 'Health Care': 21, 'Government': 17}

Revenue (object):
  Non-null: 672
  Unique: 14
  Top values: {'Unknown / Non-Applicable': 213, '$100 to $500 million (USD)': 94, '$10+ billion (USD)': 63, '$2 to $5 billion (USD)': 45, '$10 to $25 million (USD)': 41, '$1 to $2 billion (USD)': 36, '$25 to $50 million (USD)': 36, '$50 to $100 million (USD)': 31, '$1 to $5 million (USD)': 31, '-1': 27}

Competitors (object):
  Non-null: 672
  Unique: 108
  Top values: {'-1': 501, 'Roche, GlaxoSmithKline, Novartis': 10, 'Leidos, CACI International, Booz Allen Hamilton': 6, 'Los Alamos National Laboratory, Battelle, SRI International': 6, 'Oak Ridge National Laboratory, National Renewable Energy Lab, Los Alamos National Laboratory': 3, 'Nielsen, Zappi, SurveyMonkey': 3, 'Battelle, General Atomics, SAIC': 3, 'IQVIA, ICON': 3, "IMAGE Skincare, Aveda, Kiehl's": 3, 'Commerce Signals, Cardlytics, Yodlee': 3}
